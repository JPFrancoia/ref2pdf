#!/usr/bin/python3
# coding: utf-8

help = """reftool.py

Very simple script to return a DOI from a formatted citation.
Can also return a direct link to download the article from Sci-Hub.

This script uses Crossref Simple Text Query Tool:
http://www.crossref.org/SimpleTextQuery/

Usage is limited to 1000 requests per user/per month, and requires signing up
on Crossref's website. The script needs the email address you used to sign up.

Examples:
    Input: ./reftool.py myemail@mydomain.com "J.-P. Francoia, R. Pascal and L. Vial, Chem. Commun., 2015, 51, 1953"
    Output: DOI: http://dx.doi.org/10.1039/C4CC08563A

    Input: ./reftool.py -d myemail@mydomain.com "J.-P. Francoia, R. Pascal and L. Vial, Chem. Commun., 2015, 51, 1953"
    Output: Link to download the article: http://cyber.sci-hub.cc/MTAuMTAzOS9jNGNjMDg1NjNh/francoia2014.pdf

    Input: ./reftool.py -bd http://dx.doi.org/10.1039/C4CC08563A
    Output:
            @article{Francoia_2015,
            doi = {10.1039/c4cc08563a},
                url = {http://dx.doi.org/10.1039/C4CC08563A},
                year = 2015,
                publisher = {Royal Society of Chemistry ({RSC})},
                volume = {51},
                number = {10}number,
                pages = {1953--1956},
                author = {Jean-Patrick Francoia and Robert Pascalscal and Laurent Vial},
                title = {Monitoring clinical levels of heparin in human blood samples with an indicator-displacement assay},
                journal = {Chem. Commun.}
            }

            Link to download the article: http://cyber.sci-hub.cc/MTAuMTAzOS9jNGNjMDg1NjNh/francoia2014.pdf

Usage:
    reftool.py   [options] ([email] <ref_or_doi> | <email> <ref_or_doi>)

Options:
    -h --help    Display help
    -a           Option All, Equivalent to -bd
    -b           Output a bibtex entry (get information about the reference)
    -d           Output a link to download the artice from Sci-Hub (could be illegal, use at your own risk)
"""


import sys
import requests
from bs4 import BeautifulSoup, SoupStrainer
from docopt import docopt
import validators
import asyncio
# import aiohttp
from aiohttp import ClientSession
from aiohttp import TCPConnector


# Define URLs
URL_CROSSREF = "https://apps.crossref.org/SimpleTextQuery"
URL_SCIHUB = "https://sci-hub.la"
URLS_DOI_BIB = ["https://doi.org/", "http://dx.doi.org/"]

# Define the user agent
headers = {'User-agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:12.0) Gecko/20100101 Firefox/21.0'}

# Get the arguments
args = docopt(help)
email = args['<email>']
target = args['<ref_or_doi>']

# If option 'all' activated, switch the other options
if args['-a']:
    args['-b'] = True
    args['-d'] = True


def extract_doi(url_doi: str):

    if validators.url(url_doi) and '10.1' in url_doi:
        for url in URLS_DOI_BIB:
            if url in url_doi:
                doi = url_doi.split(url)[-1]
                return doi
    else:
        return None


def get_doi_locally(target: str):

    """"Test if the input is a doi. Returns it or returns None"""

    if target.startswith('10.1'):
        return target
    elif validators.url(target) and '10.1' in target:
        doi = extract_doi(target)
        return doi
    else:
        return None


def get_doi_remotely(email: str, target: str):

    if email is None:
        print("No email provided, exiting")
        return None

    # First, get the form and start a session. This is necessary, posting
    # directly the parameters doesn't work
    s = requests.session()
    s.get(URL_CROSSREF, headers=headers)

    payload = {'email': email,
               'command': 'Submit',
               'freetext': target}

    # Post the form
    try:
        req = s.post(URL_CROSSREF, data=payload, headers=headers)
    except Exception as e:
        print("Error contacting the server: {}".format(e))
        return None

    # with open("out.html", "w") as f:
        # f.write(req.text)

    # Parse the result
    strainer = SoupStrainer("td", attrs={"class": "resultB"})
    soup = BeautifulSoup(req.text, "html.parser", parse_only=strainer)

    try:
        doi = extract_doi(soup.a.text)
        return doi
    except Exception as e:
        print("reftool could not get the DOI from your reference.")
        print("A possible reason is an unregistered email address.")
        print("It could also be an invalid input.")
        print("Error: {}".format(e))
        return None


async def get_bibtex(doi):

    """Get the bibtex entry"""

    headers = {"Accept": "application/x-bibtex"}
    # r = await requests.get(URLS_DOI_BIB[0] + doi, headers=headers)
    async with ClientSession(connector=TCPConnector(verify_ssl=False)) as session:
        async with session.get(URLS_DOI_BIB[0] + doi, headers=headers) as r:
            content = await r.text()

    # r = await aiohttp.request('GET', URLS_DOI_BIB[0] + doi, headers=headers)

    if "Error: DOI Not Found" in content:
        print("DOI not found. Are you sure you didn't make a typo ?")
        print("Impossible to generate a bibtex entry\n")
    else:
        print(content + "\n")

    # headers['Accept'] = ""


async def get_url_scihub(doi):

    """Get the URL of the paper from Sci-Hub"""

    # Post the form
    payload = {'sci-hub-plugin-check': '', 'request': doi}

    async with ClientSession(connector=TCPConnector(verify_ssl=False)) as session:
        async with session.post(URL_SCIHUB, data=payload, headers=headers) as r:
            content = await r.text()
    # req = requests.post(URL_SCIHUB, data=payload, headers=headers)

    # Past the result to get the link of the paper
    strainer = SoupStrainer("iframe", attrs={"id": "pdf"})
    soup = BeautifulSoup(content, "lxml", parse_only=strainer)

    try:
        link_to_pdf = soup.iframe['src']

        if not link_to_pdf.startswith('http:'):
            link_to_pdf = 'http:' + link_to_pdf
        print('Link to download the article: {}'.format(link_to_pdf))
    except TypeError:
        print("Error with SciHub. This paper is probably not available.")
    except Exception as e:
        print("Unknown error with SciHub:")
        print(e)




if __name__ == '__main__':

    doi = get_doi_locally(target)

    if doi is not None and not args['-b'] and not args['-d']:
        print("Nothing to do, exiting")
        sys.exit()

    if doi is None:
        doi = get_doi_remotely(email, target)

    print(doi)

    tasks = []
    ioloop = asyncio.get_event_loop()

    if args['-b']:
        task = asyncio.ensure_future(get_bibtex(doi))
        tasks.append(task)

    if args['-d']:
        task = asyncio.ensure_future(get_url_scihub(doi))
        tasks.append(task)

    ioloop.run_until_complete(asyncio.wait(tasks))
    ioloop.close()
